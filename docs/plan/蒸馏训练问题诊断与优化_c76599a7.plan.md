---
name: 蒸馏训练问题诊断与优化
overview: 分析当前蒸馏训练结果（3 epoch）并提出优化策略。核心结论：3 epoch 远未收敛，训练应继续跑完 80 epoch；同时可考虑从教师权重部分初始化学生来加速收敛。
todos:
  - id: wait-80-epoch
    content: 让当前蒸馏训练跑完 80 epoch，然后用 evaluate.sh 评测 epoch 79 和 avg_30
    status: pending
  - id: impl-init-from-teacher
    content: 在 train_distill.py 中实现 --init_from_teacher 功能（从教师权重截取切片初始化学生），并在 run_distill.sh 中加入对应参数
    status: pending
isProject: false
---

# 蒸馏训练结果诊断与优化方案

## 1. 核心诊断：3 epoch 远未收敛，不必恐慌

### CV Loss 对比（最关键的指标）

- 教师 (top20_weight_surgery, **从预训练 base.pt 初始化**)
  - Epoch 0: cv_loss = **15.65**
  - Epoch 3: cv_loss = 14.47
  - Epoch 10: cv_loss = 13.82
  - Epoch 79: cv_loss = **12.91** (最终)
  - 80 epoch 总计下降 **2.74** 点

- 学生 (distill_mini, **从随机初始化**)
  - Epoch 0: cv_loss = **21.68**
  - Epoch 3: cv_loss = 19.73
  - Epoch 5: cv_loss = **18.94**
  - 仅 5 epoch 已下降 **2.74** 点（与教师 80 epoch 的下降量相当）

**学生的学习速度其实非常快**（有 KD loss 帮助），但起点差距太大（21.68 vs 15.65），差了约 6 个点。这 6 个点的差距来自**教师有预训练权重初始化而学生是随机初始化**。

### CV Accuracy 趋势

```
Epoch 0: 39.8%  →  Epoch 3: 53.1%  →  Epoch 5: 55.3%  (还在快速提升)
```

CV accuracy 每 epoch 提升约 2-3%，按此趋势 80 epoch 后应能达到合理水平。

### 训练 Loss 趋势

```
Epoch 0 Batch 0:  ctc=233.5  kd=2.31
Epoch 3 Batch 170: ctc=12.9   kd=0.38
Epoch 5 Batch 170: ctc=12.9   kd=0.35
```

CTC loss 从 233 降到 13 左右，KD loss 从 2.3 降到 0.35，两者都在正常收敛。

## 2. 置信度分数的差异解释

| 模型 | output_dim | 嗨小问置信度 (正样本) | 原因 |

|------|-----------|---------------------|------|

| top440_weight_surgery | 440 | ~0.999 | 440 类 softmax 极尖锐，非关键词类吸收 probability mass |

| top20_weight_surgery (teacher, epoch 79) | 20 | ~0.62 | 20 类空间较小，blank 竞争更强 |

| distill_mini (student, epoch 3) | 20 | ~0.37-0.41 | 随机初始化仅训 3 epoch，分布远未收敛 |

**top440 和 top20 的置信度差异是 output_dim 不同导致的结构性差异，不是模型质量问题。** top20 教师的 0.62 置信度配合低阈值 (0.039)，FRR 低至 1.12%。

学生的 0.37-0.41 随着训练将逐步向教师的 0.62 靠拢。

## 3. 推荐方案

### 方案 A（首选）：继续跑完 80 epoch，不改任何东西

当前训练正在正常收敛，loss 和 accuracy 趋势都很健康。建议：

- 让当前训练跑完全部 80 epoch
- 评测 epoch 79（或 avg_30），预期 FRR 应大幅改善
- 这是最低风险的方案

### 方案 B（加速收敛）：从教师权重部分初始化学生

当前学生的最大劣势是随机初始化。教师和学生的 FSMN 结构相同，只是维度不同。可以通过**截断/切片教师权重**来初始化学生，核心思路：

对于每一层，如果教师是 `(out_dim_t, in_dim_t)`，学生是 `(out_dim_s, in_dim_s)`，取教师权重的前 `[:out_dim_s, :in_dim_s]` 切片来初始化。

需要初始化的层和维度匹配情况：

- `in_linear1`: 教师 (140, 400), 学生 (96, 400) -- 可切片前 96 行
- `in_linear2`: 教师 (250, 140), 学生 (160, 96) -- 可切片前 160x96
- FSMN blocks: 教师 4 层，学生 3 层
  - block 0-2 的 `LinearTransform`: 教师 (128, 250), 学生 (64, 160) -- 可切片
  - block 0-2 的 `FSMNBlock` depthwise conv: 教师 (128, ...), 学生 (64, ...) -- 可切片前 64 个 channel
  - block 0-2 的 `AffineTransform`: 教师 (250, 128), 学生 (160, 64) -- 可切片
- `out_linear1`: 教师 (140, 250), 学生 (96, 160) -- 可切片
- `out_linear2`: 教师 (20, 140), 学生 (20, 96) -- 可切片前 96 列

实现方式：在 [`wekws/bin/train_distill.py`](wekws/bin/train_distill.py) 中新增一个 `--init_from_teacher` 参数。当指定时，在创建学生模型后、开始训练前，从教师权重中截取对应切片来初始化学生。

这个优化预期效果：将学生的初始 cv_loss 从 ~21.7 拉近到 ~17-18，大幅加速收敛。

### 方案 C（如果 80 epoch 后仍不满意）：减小压缩力度

尝试一个中间尺寸的学生网络：

- `num_layers=4` (与教师相同), `linear_dim=192`, `proj_dim=96`, `input/output_affine_dim=112`
- 预估参数 ~220K（介于教师 392K 和当前学生 136K 之间）
- 只需新建一个 config yaml 即可

## 4. 建议的行动路径

```
1. 当前训练继续跑完 80 epoch（不要停！）
     ↓
2. 评测 epoch 79 和 avg_30 的结果
     ↓
3a. 如果 FRR < 5%:      成功！与教师对比后决定是否需要进一步优化
3b. 如果 FRR 5%~15%:    实现方案 B（教师权重部分初始化），重新训练
3c. 如果 FRR > 15%:     先试方案 B，若仍不行则用方案 C（更大学生）
```

## 5. 实施方案 B 的代码改动

仅需修改 1 个文件：[`wekws/bin/train_distill.py`](wekws/bin/train_distill.py)

新增函数 `_init_student_from_teacher(student_model, teacher_model)`:

```python
def _init_student_from_teacher(student_model, teacher_model):
    """从教师权重截取切片来初始化学生（维度不同时取前 N 行/列）"""
    s_state = student_model.state_dict()
    t_state = teacher_model.state_dict()
    initialized = []
    for key in s_state:
        if key in t_state:
            s_shape = s_state[key].shape
            t_shape = t_state[key].shape
            if s_shape == t_shape:
                s_state[key] = t_state[key].clone()
                initialized.append(key)
            elif len(s_shape) == len(t_shape) and all(
                    s <= t for s, t in zip(s_shape, t_shape)):
                slices = tuple(slice(0, s) for s in s_shape)
                s_state[key] = t_state[key][slices].clone()
                initialized.append(f"{key} (sliced)")
    student_model.load_state_dict(s_state)
    logging.info("Initialized %d params from teacher: %s",
                 len(initialized), initialized)
```

新增命令行参数 `--init_from_teacher`（flag），在 shell 脚本中可选传入。